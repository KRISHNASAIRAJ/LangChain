{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"LangChain/.env\")\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY is not set in the environment variables.\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello, world! It's great to meet you! Is there something I can help you with, or would you like to chat?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 14, 'total_tokens': 42, 'completion_time': 0.023333333, 'prompt_time': 0.002824558, 'queue_time': 0.376538945, 'total_time': 0.026157891}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None} id='run-5b11c4e1-539a-4271-9cc7-f20fa45ee6eb-0' usage_metadata={'input_tokens': 14, 'output_tokens': 28, 'total_tokens': 42}\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke function is used to generate text based on the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='హి! మీ స్థితి ఎలా?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 30, 'total_tokens': 59, 'completion_time': 0.024166667, 'prompt_time': 0.00584942, 'queue_time': 0.019047966, 'total_time': 0.030016087}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-d4c5a20a-3a7f-4f7f-ab13-e9dd62808504-0', usage_metadata={'input_tokens': 30, 'output_tokens': 29, 'total_tokens': 59})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Telugu\"),\n",
    "    HumanMessage(\"Hi! How are you doing ?\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that ChatModels receive message objects as input and generate message objects as output.\n",
    "\n",
    "#### LangChain also supports chat model inputs via strings or OpenAI format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 11, 'total_tokens': 37, 'completion_time': 0.021666667, 'prompt_time': 0.001747953, 'queue_time': 0.018592597, 'total_time': 0.02341462}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a1488dc3-f2bc-48d5-90d6-0c46739036fd-0', usage_metadata={'input_tokens': 11, 'output_tokens': 26, 'total_tokens': 37})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello\")\n",
    "\n",
    "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
    "\n",
    "model.invoke([HumanMessage(\"Hello\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|హ|ల|ో|!| మ|ీ|ర|ు| ఎ|ల|ా| ఉ|న|్|న|ా|ర|ు|?||"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In LangChain, Prompt Templates are reusable and structured templates that help in generating prompts for Large Language Models (LLMs). They are a crucial feature for prompt engineering, as they allow you to create well-designed, parameterized prompts for various use cases, ensuring consistency and flexibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Prompt Template?\n",
    "A Prompt Template in LangChain is a string with placeholders (variables) that can be dynamically replaced with specific input values at runtime. This makes it easier to customize prompts for different inputs while keeping the general structure of the prompt intact.\n",
    "\n",
    "template = \"Translate the following text to French: {text}\"\n",
    "\n",
    "Here, {text} is a placeholder that can be replaced with actual input when the template is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Use Prompt Templates?\n",
    "##### Reusability: Templates allow you to reuse prompt structures across different inputs and tasks.\n",
    "##### Consistency: Ensures that prompts maintain a consistent structure, reducing variability in LLM responses.\n",
    "##### Parameterization: Makes it easy to insert dynamic content into prompts.\n",
    "##### Readability: Keeps the prompt logic clean and separated from the code.\n",
    "\n",
    "### Components of Prompt Templates in LangChain:\n",
    "##### Base string with placeholders (e.g., \"{name}, can you summarize {topic} for me?\").\n",
    "##### Input Variables: The variables that need to be filled when the template is used (e.g., name and topic in the example above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Telugu', additional_kwargs={}, response_metadata={}), HumanMessage(content='What Curry?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Telugu\", \"text\": \"What Curry?\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ఏం కర్రీ? (Eṁ karrī?)\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
